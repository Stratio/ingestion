NASA Apache Logs Demo
*********************

To play around with the different components that we have created within Stratio Ingestion we have designed an scenario where we launch one flume agent against an Ubuntu Virtual Machine with the following structure.
We will have a data source (apache logs from NASA in 1995) which will be read and send to three sinks (Cassandra Database, ElasticSearch and Stratio Streaming) throw three channels. We sill need this components:

* Cassandra database
* Stratio Streaming Engine
* Zookeeper (for Stratio Streaming)
* Apache kafka (for Stratio Streaming)
* ElasticSearch
* Kibana (for data visualization)


Agents configuration
====================

By running the agent, flume will walk through the apache log file and for each log entry it will use a morphline interceptor (see interceptor.conf file) to read the lines, append an autogenerated UUID, and parse the log fields using grok.

The full agent configuration is the following (see flume-conf.properties):

* Source:

  - SpoolDir (we have used a 300 Mb apache log file)

* Channels:

  - 3 file channels for cassandra, stratio streaming & elastic search

* Sinks:

  - Cassandra sink (`developed by Stratio`_), see also the definition_access_log.json attached)
  - Stratio Streaming sink (`also developed by Stratio`_)
  - Elastic search sink (included in flume core)

.. _developed by Stratio: https://github.com/Stratio/flume-ingestion/tree/master/stratio-sinks/stratio-cassandra-sink
.. _also developed by Stratio:  https://github.com/Stratio/flume-ingestion/tree/master/stratio-sinks/stratio-stratiostreaming-sink



Preparing the environment
=========================

Manually
--------

You need an extracted full distribution of Stratio Ingestion at ``/opt/sds/ingestion``. You can use a different path via the
``INGESTION_HOME`` environment variable. You will also need a running Stratio Streaming if you use the sink (you can use our sandbox for that), ElasticSearch and Cassandra. By default, it will use only
Cassandra and ElasticSearch. See below for different set ups you can use.


Vagrant Box
-----------

You can use our Vagrant Box to run the example, just type: ``vagrant up stratio/ingestion`` to get our sandbox.

You can edit the conf/flume-conf.properties for customizing the example. By default, we have activated two sinks: ElasticSearch and Cassandra, but we provide the configuration for Stratio Streaming Sink commented in the same file.


Running the example
===================

To run the agent just type:
::

   sudo ./opt/sds/ingestion/examples/apache_logs/bin/run_example.sh

This command downloads an apache log and sends it to Stratio Ingestion, you can check the sinks, channels and sources configurated in http://IP:3545/metrics (you can obtain the IP of your machine when you start the vagrant box). This metrics shows you the flow of the info throw the flume agent (source -> channel -> sink)

Since we have activated two sink (ElasticSearch and Cassandra) you can check the info loaded in them:

- Cassandra: Enter in cassandra console
::

    sudo ./opt/sds/cassandra/bin/cqlsh

 then type
::

    select * from test.access_logs;

- ElasticSearch: Enter in Kibana http://IP:5601 in Dashboard Tab You can Load saved dashboard and select "Apache Logs Demo Dashboard". Due to date of logs, you must select a new time filter between July-1995 and Agoust-1995.


Conection with Stratio Streaming
================================

If you want to check the conection with Stratio Streaming uncomment the configuration in con/flume-conf.properties we recommend to use straming vagrant box (``vagrant up stratio/streaming``) and use this parameters to configure flume (IPs and port for kafka and zookeeper). If you have streaming vagrant box working, then you can run the flume example.
The example will create the stream necesary for working in streaming, you can enter in streaming shell and type create new querys:

Create query **host_requests_per_second**:

::

    add query --stream testStream --definition "from testStream #window.time(1 second)
    select count(log_id) as host_requests_per_second,log_id, log_host,log_user,log_date,
    log_http_method, log_url_path, log_http_version, log_http_code, log_bytes_returned
    group by log_host insert into host_requests_per_second"

Create query **resource_requests_per_second**:

::

    add query --stream testStream --definition "from testStream #window.time(1 second)
    select count(log_id) as resource_requests_per_second,log_id, log_host,log_user,
    log_date,log_http_method, log_url_path, log_http_version, log_http_code,
    log_bytes_returned group by log_url_path insert into resource_requests_per_second"

Create query **host_request_per_seconde_per_resource**:

::

    add query --stream testStream --definition "from testStream #window.time(1 second)
    select count(log_id) as host_request_per_second_per_resource,log_id,
    log_host,log_user,log_date,log_http_method, log_url_path, log_http_version,
    log_http_code, log_bytes_returned group by log_host,log_url_path insert
    into host_request_per_second_per_resource"

Index all streams (so you can check them in ElasticSearch):

::

    index start --stream testStream

::

    index start --stream host_requests_per_second

::

    index start --stream resource_requests_per_second

::

    index start --stream host_request_per_second_per_resource
